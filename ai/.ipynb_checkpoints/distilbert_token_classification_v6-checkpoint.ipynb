{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e3020e9eb09a9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
    "\n",
    "label_list = [\"O\", \"B-DEP\", \"I-DEP\", \"B-ARR\", \"I-ARR\"]\n",
    "\n",
    "path = \"data/token_classification/\"\n",
    "\n",
    "# dataset dict with emtpy attribute train\n",
    "dataset = DatasetDict({'train': Dataset.from_dict({})})\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        new_data = load_dataset('csv', data_files=path + file, delimiter=';')\n",
    "        dataset[\"train\"] = concatenate_datasets([dataset[\"train\"], new_data[\"train\"]])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d31bd4977ff05",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate tokens and ner_tags as lists\n",
    "dataset = dataset.map(lambda line: {'tokens': ast.literal_eval(line['tokens'])})\n",
    "dataset = dataset.map(lambda line: {'ner_tags': ast.literal_eval(line['ner_tags'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767945c7a81ad58",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cast ner_tags to ClassLabel with all labels present in ner_tags\n",
    "from datasets import ClassLabel, Sequence\n",
    "\n",
    "dataset = dataset.cast_column(\"ner_tags\", Sequence(feature=ClassLabel(num_classes=len(label_list), names=label_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffac0966310b7b3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8366b049c9f097",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b53ae63504a1d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d854dbfadcd2196",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count number of each label in ner_tags\n",
    "from collections import Counter\n",
    "\n",
    "tag_counts = Counter()\n",
    "for tags in dataset['train']['ner_tags']:\n",
    "    tag_counts.update(tags)\n",
    "    \n",
    "tag_counts = {label_list[key]: value for key, value in tag_counts.items()}\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125be68101ed46f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(tag_counts.keys(), tag_counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee5601cb35cac9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del tag_counts['O']\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.bar(tag_counts.keys(), tag_counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b748ce559246c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle train dataset, and pick 75% of it\n",
    "train_data = dataset['train']\n",
    "train_data = train_data.shuffle(seed=42)\n",
    "train_data = train_data.train_test_split(test_size=0.25)['train']\n",
    "\n",
    "# Split test dataset into 10% validation and 10% test\n",
    "train_test_valid = train_data.train_test_split(test_size=0.2)\n",
    "test_valid = train_test_valid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a0c5666107ccb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40f9cf5547a573",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54195c59cdcc187f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50851fde289728b2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ea6c51fd36c5b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699448cb4c1f46c5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer(example[\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ff719508c454",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba98f777573cf13",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39a004573b40ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae005e07fce1246",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec2072b75fee3f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40541747737267",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc826e8b9f7f89",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce333710b79d837",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 3\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce9221697b63ea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"models/distilbert-finetuned-token-classification-ner-trip\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512bb6a5253ad47",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d6cd897d5cf5fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f2114b0a6931",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999a473af436906",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    y_pred = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    y_true = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=y_pred, references=y_true)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7217f2e813c589",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63bb885915a419f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaced7e31b8d651",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/distilbert-finetuned-token-classification-ner-trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e96dc23c719a9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_metrics = trainer.evaluate(tokenized_datasets[\"train\"])\n",
    "validation_metrics = trainer.evaluate(tokenized_datasets[\"valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17869fd8a7122d08",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"valid\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "y_pred = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "y_true = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=y_pred, references=y_true)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186e492a8234a39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237844430a7a0f7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "flat_y_true = [item for sublist in y_true for item in sublist]\n",
    "flat_y_pred = [item for sublist in y_pred for item in sublist]\n",
    "\n",
    "cm = confusion_matrix(flat_y_true, flat_y_pred, labels=label_list)\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=label_list, yticklabels=label_list)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa939002436b7c1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Matrice de confusion sans le label O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942db4c4570d654",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_label(label_to_exclude, true_labels, predicted_labels):\n",
    "    filtered_true = [true for true, pred in zip(true_labels, predicted_labels) if pred != label_to_exclude]\n",
    "    filtered_pred = [pred for pred in predicted_labels if pred != label_to_exclude]\n",
    "    return filtered_true, filtered_pred\n",
    "\n",
    "filtered_y_true, filtered_y_pred = filter_label(\"O\", y_true, y_pred)\n",
    "\n",
    "flat_filtered_y_true = [item for sublist in filtered_y_true for item in sublist]\n",
    "flat_filtered_y_pred = [item for sublist in filtered_y_pred for item in sublist]\n",
    "\n",
    "cm = confusion_matrix(flat_filtered_y_true, flat_filtered_y_pred, labels=label_list[1:])\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=label_list[1:], yticklabels=label_list[1:])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b17bb00596a5eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(flat_y_true, flat_y_pred, labels=label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da64b4d3f078a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ff2247d82c1c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {\n",
    "    'eval_accuracy': 'Accuracy',\n",
    "    'eval_loss': 'Loss',\n",
    "    'eval_f1': 'F1',\n",
    "}\n",
    "\n",
    "for metric, title in metrics.items():\n",
    "\n",
    "    points = []\n",
    "\n",
    "    for epoch in trainer.state.log_history:\n",
    "        if metric in epoch.keys():\n",
    "            points.append(epoch[metric])\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(points)\n",
    "    plt.title(f\"{title} per epoch (min / max: {min(points):.2f} / {max(points):.2f})\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(points)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(f\"{title} per epoch (between 0 and 1)\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f36e0bef37a8a5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gather_outputs(outputs: list) -> list:\n",
    "    # Group entities by their sequence\n",
    "    grouped_entities = []\n",
    "    current_group = []\n",
    "    for entity in outputs:\n",
    "        if not current_group or entity['start'] == current_group[-1]['end']:\n",
    "            current_group.append(entity)\n",
    "        else:\n",
    "            grouped_entities.append(current_group)\n",
    "            current_group = [entity]\n",
    "    \n",
    "    # Append the last group\n",
    "    if current_group:\n",
    "        grouped_entities.append(current_group)\n",
    "    \n",
    "    return grouped_entities\n",
    "\n",
    "def get_locations_from_outputs(sentence: str, outputs: list) -> list:\n",
    "    groups = gather_outputs(outputs)\n",
    "    groups = [{\"group\": group[0][\"entity_group\"], \"city\": sentence[group[0][\"start\"]:group[-1][\"end\"]] } for group in groups]\n",
    "    \n",
    "    # if there is more than one of the same entity group, return groups\n",
    "    if len(set([group[\"group\"] for group in groups])) != len(groups):\n",
    "        return groups\n",
    "    \n",
    "    return sorted(groups, key=lambda group: group[\"group\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105eba05962a6a9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentences = [\n",
    "    \"Je veux aller de Port-Boulet à Le Havre.\",\n",
    "    \"Peux-tu m'aider à trouver mon chemin vers Paris en partant d'Épierre ?\",\n",
    "    \"Je cherche un moyen d'aller de Margny-Lès-Compiègne à Saarbrücken /Sarrebruck.\",\n",
    "    \"Je veux me rendre chez mon ami Etienne à Saint-Étienne depuis Nantes.\",\n",
    "    \"Je veux aller de la ville de Marseille à Tours.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    token_classifier = pipeline(\"token-classification\", model=\"models/distilbert-finetuned-token-classification-ner-trip\", aggregation_strategy=\"simple\")\n",
    "    outputs = token_classifier(sentence)\n",
    "    print(get_locations_from_outputs(sentence, outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
